This repository demonstrates a step-by-step adaptation of a pre-trained LLM on the TREC question‐classification dataset, beginning with zero-shot prompting to illustrate its out-of-the-box capabilities, proceeding to few-shot prompting to show how a handful of examples can nudge its behavior, and culminating in supervised fine-tuning to unlock reliable question‐type classification (e.g. ‘DESC’, ‘LOC’, ‘HUM’) and accurate entity extraction—capabilities that remain inconsistent or absent before fine-tuning but emerge clearly once the model has been explicitly trained on task-specific labels.
